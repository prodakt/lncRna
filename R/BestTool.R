#' Calculate Performance Statistics for Individual Tools (Refactored)
#'
#' Computes confusion matrix statistics (accuracy, precision, recall, etc.)
#' for selected coding potential prediction tools using results prepared by
#' `SumSingleTools`. Allows interactive selection if specific tools are not provided.
#' Requires the 'caret' package. This version uses internal helper functions.
#'
#' @param SumSingleTools_list A list generated by `SumSingleTools()`, containing
#'                            `$tools` (list of 0/1 prediction vectors), and
#'                            `$isNC` (numeric vector of true labels: 1=nc, 0=cds).
#' @param tools An optional character vector specifying the names of the tools
#'              (from `names(SumSingleTools_list$tools)`) to analyze. If `NULL` (default),
#'              the user will be prompted to select tools interactively.
#' @param digits The number of decimal places to round the final statistics to (default: 4).
#'
#' @return A data frame where rows are performance metrics (combined overall and by-class)
#'         and columns correspond to the selected tools, rounded to the specified
#'         number of digits. Returns `NULL` if validation fails, no tools are selected,
#'         or no statistics can be calculated.
#' @keywords confusion matrix accuracy performance caret lncRNA validation statistics
#' @export
#' @examples
#' # Ensure 'caret' is installed for examples to run:
#' if (requireNamespace("caret", quietly = TRUE)) {
#'
#'   # --- Create Example Data (mimicking SumSingleTools output) ---
#'   set.seed(123) # for reproducibility
#'   n_seq <- 50
#'   example_summary_list <- list(
#'     seqIDs = paste0("Seq", 1:n_seq),
#'     tools = list(
#'       CPC2 = sample(c(0, 1), n_seq, replace = TRUE, prob = c(0.3, 0.7)),
#'       CPAT = sample(c(0, 1), n_seq, replace = TRUE, prob = c(0.4, 0.6)),
#'       PLEK = sample(c(0, 1), n_seq, replace = TRUE, prob = c(0.5, 0.5)),
#'       # Add a tool that might produce NAs for some metrics (e.g., predicts only one class)
#'       FEELnc = rep(1, n_seq)
#'     ),
#'     type = sample(c("nc", "cds"), n_seq, replace = TRUE),
#'     isNC = NULL, # Will be derived from 'type'
#'     sums = NULL    # Will be derived from 'tools'
#'   )
#'   # Derive isNC from type
#'   example_summary_list$isNC <- ifelse(example_summary_list$type == "nc", 1, 0)
#'   # Derive sums from tools
#'   example_summary_list$sums <- rowSums(do.call(cbind, example_summary_list$tools[1:3])) # Sum first 3
#'
#'   # --- Example 1: Analyze specific tools ---
#'   selected_tool_names <- c("CPC2", "CPAT", "FEELnc")
#'   performance_stats_specific <- BestTool(SumSingleTools_list = example_summary_list,
#'                                          tools = selected_tool_names)
#'   print("Performance for CPC2, CPAT, FEELnc (rounded):")
#'   print(performance_stats_specific)
#'
#'   # --- Example 2: Simulate non-interactive selection ---
#'   # When run non-interactively, the helper function .select_tools_interactively
#'   # will default to selecting the first tool.
#'   cat("\nSimulating non-interactive run (will select first tool):\n")
#'   performance_stats_noninteractive <- BestTool(SumSingleTools_list = example_summary_list,
#'                                                tools = NULL) # Pass NULL
#'   print(performance_stats_noninteractive)
#'
#' } else {
#'   message("Please install the 'caret' package to run BestTool examples.")
#' }
#'
BestTool <- function(SumSingleTools_list, tools = NULL, digits = 4) {

  # --- 1. Input Validation ---
  if (!.validate_besttool_input(SumSingleTools_list)) {
    return(NULL) # Stop if basic validation fails (warnings issued by helper)
  }

  available_tools <- names(SumSingleTools_list$tools)
  reference_labels <- SumSingleTools_list$isNC

  # --- 2. Tool Selection ---
  if (is.null(tools)) {
    selected_tools <- .select_tools_interactively(available_tools)
  } else {
    selected_tools <- .validate_tool_names(tools, available_tools)
  }

  # Check if any tools remain after selection/validation
  if (is.null(selected_tools) || length(selected_tools) == 0) {
    warning("No valid tools selected or available for analysis. Returning NULL.", call. = FALSE)
    return(NULL)
  }

  # --- 3. Calculate Statistics for Selected Tools ---
  results_list <- list() # Will store named vectors from helper
  # Prepare reference factor once
  ref_factor <- factor(reference_labels, levels = c("0", "1"))

  for (tool_name in selected_tools) {
    tool_predictions <- SumSingleTools_list$tools[[tool_name]]
    # Calculate stats using the helper function, returns a named vector or NULL
    single_tool_stats_vector <- .calculate_single_tool_stats(tool_name, tool_predictions, ref_factor)

    if (!is.null(single_tool_stats_vector)) {
      results_list[[tool_name]] <- single_tool_stats_vector
    }
    # Warnings for failed calculations are handled within the helper
  }

  # --- 4. Format Output ---
  if (length(results_list) == 0) {
    warning("No statistics could be calculated for the selected tools.", call. = FALSE)
    return(NULL)
  }

  # --- Ensure Consistent Rows before cbind ---
  # Find the union of all metric names (potential row names) across all tools
  all_metric_names <- unique(unlist(lapply(results_list, names)))
  if (is.null(all_metric_names)) {
    warning("Could not determine metric names. Returning NULL.", call. = FALSE)
    return(NULL)
  }


  # Create a list of data frames, ensuring each has all metrics
  aligned_results_list <- lapply(results_list, function(tool_stats_vector) {
    # Create a template vector with NAs for all possible metrics
    template <- rep(NA_real_, length(all_metric_names))
    names(template) <- all_metric_names

    # Fill the template with the actual values from the tool's results
    common_metrics <- intersect(names(tool_stats_vector), all_metric_names)
    if(length(common_metrics) > 0){
      template[common_metrics] <- tool_stats_vector[common_metrics]
    }

    # Convert the full template vector to a single-column data frame
    # Ensure row names are set correctly from the template names
    data.frame(MetricValue = template, row.names = all_metric_names)
  })

  # Combine the aligned data frames into a single data frame
  # Now cbind should work as all data frames have the same rows (metrics)
  final_results_df <- do.call(cbind, aligned_results_list)

  # Set column names explicitly to the tool names
  colnames(final_results_df) <- names(aligned_results_list) # Use names from the aligned list

  # Round the numeric values in the final data frame
  # Use is.numeric to avoid trying to round potential non-numeric rows if errors occurred
  # This check might be redundant now but is safe.
  # Apply rounding using lapply across columns
  final_results_df <- as.data.frame(lapply(final_results_df, function(col) {
    if(is.numeric(col)) round(col, digits = digits) else col
  }))
  # Ensure row names are preserved after lapply
  rownames(final_results_df) <- all_metric_names


  return(final_results_df)
}

#' Calculate Statistics for a Single Tool
#'
#' Computes confusion matrix statistics for one tool using caret. Returns a named
#' vector containing all available metrics, including NAs.
#'
#' @param tool_name The name of the tool to analyze.
#' @param predictions A numeric vector (0/1) of predictions for the tool.
#' @param reference_factor A factor vector of true labels (levels "0", "1").
#' @return A named numeric vector containing combined overall and by-class statistics
#'         for the tool (may include NAs), or NULL if calculation fails catastrophically.
#' @keywords internal statistics helper caret confusion matrix
#' @importFrom caret confusionMatrix
#'
.calculate_single_tool_stats <- function(tool_name, predictions, reference_factor) {
  # Check prediction validity
  if (is.null(predictions) || length(predictions) != length(reference_factor)) {
    warning(paste("Skipping tool '", tool_name, "' due to missing or mismatched predictions."), call. = FALSE)
    return(NULL)
  }
  if (!all(predictions %in% c(0, 1))) {
    warning(paste("Skipping tool '", tool_name, "' as predictions are not all 0 or 1."), call. = FALSE)
    return(NULL)
  }

  dat_factor <- factor(predictions, levels = levels(reference_factor)) # Use same levels as reference

  # Use tryCatch for robustness
  cm_result <- tryCatch({
    caret::confusionMatrix(data = dat_factor,
                           reference = reference_factor,
                           mode = "everything", # Get all available stats
                           positive = "1") # Assume '1' (non-coding) is positive class
  }, error = function(e) {
    warning(paste("Could not compute confusion matrix for tool '", tool_name, "'. Error: ", e$message), call. = FALSE)
    return(NULL) # Return NULL if confusionMatrix itself fails
  })

  if (is.null(cm_result)) {
    return(NULL)
  }

  # Extract and combine stats
  stats_overall <- cm_result$overall
  stats_byClass <- cm_result$byClass
  # Combine into a single named vector
  combined_stats_vector <- c(stats_overall, stats_byClass)

  # Ensure the output is numeric, converting potential non-numeric outputs to NA
  # This handles cases where caret might return characters like "NaN"
  numeric_stats_vector <- suppressWarnings(as.numeric(combined_stats_vector))
  names(numeric_stats_vector) <- names(combined_stats_vector)

  return(numeric_stats_vector) # Return the named vector (may contain NAs)
}
